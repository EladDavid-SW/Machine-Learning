{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ex1.ipynb","provenance":[],"collapsed_sections":["izRyOIxK7xHn"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"izRyOIxK7xHn"},"source":["# **Ex1: Linear Regression and methods for finding the minimum**\n","\n","---\n","\n","Elad David (206760274) & Inbar Shmaya (208774026)"]},{"cell_type":"code","metadata":{"id":"VUrz_Lh87zyV","executionInfo":{"status":"ok","timestamp":1632670173745,"user_tz":-180,"elapsed":23,"user":{"displayName":"Elad David","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAAlScA5LNx4a_8K3ad_Q_ugHl3nMQR0bCFHq7=s64","userId":"14654848345289467053"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Stop condition variables:\n","epsilon = 0.0000001  # 10 ^ -8\n","delta = 0.0001 # 10 ^ -5\n","max_iterations = 150"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_fgHBOIf72Ul"},"source":["# **Question 1:** Load data file and initialize matrix X and vector y"]},{"cell_type":"code","metadata":{"id":"CXRC0H0Q79TY","executionInfo":{"status":"error","timestamp":1632670174052,"user_tz":-180,"elapsed":327,"user":{"displayName":"Elad David","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAAlScA5LNx4a_8K3ad_Q_ugHl3nMQR0bCFHq7=s64","userId":"14654848345289467053"}},"outputId":"1c603108-f01e-41a6-8919-bde614d71a16","colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["f = np.loadtxt(open(\"/content/drive/MyDrive/ML/EX1/data.csv\", 'r'), delimiter = \",\") # Load the data from the file\n","\n","numOfRows = len(f)  \n","numOfCols = len(f[0]) \n","\n","# Padd with 1's vector\n","oneVector = np.ones((numOfRows, 1)) # For theta_0 variable\n","for i in oneVector: # np.ones becomes a vector of NANs in the next code scope\n","  i = 1\n","f = np.column_stack((oneVector, f))\n","data = f.T # Transpose the matrix-data in the file\n","\n","x = data[0 : numOfCols - 1]\n","y = data[numOfCols -1 : numOfCols][0]\n","m = len(x[0]) # num of samples\n","n = len(x) - 1 # num of features (not including the 1's column)"],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-ebb311a40bdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/ML/EX1/data.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Load the data from the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnumOfRows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnumOfCols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"qAqznnKgHRAq"},"source":["# **Section A:** Normalize the data (using the normal-distribution method)"]},{"cell_type":"code","metadata":{"id":"5iFX2V9THUHQ","executionInfo":{"status":"aborted","timestamp":1632670174038,"user_tz":-180,"elapsed":305,"user":{"displayName":"Elad David","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAAlScA5LNx4a_8K3ad_Q_ugHl3nMQR0bCFHq7=s64","userId":"14654848345289467053"}}},"source":["def normalize_col(v):\n","  \"\"\"\n","  Return normalized column using the normal-distribution method\n","  \"\"\"\n","  ave = np.average(v)  # Calculate average\n","  std = np.nanstd(v)   # Calculate Standard-deviation\n","    \n","  # Avoid \"division in 0\" error\n","  # If std = 0 --> There is no need to normalize (no changes in the variables) \n","  if std == 0: # Happens when all the values in the column are equal (like the 1's column)\n","    print(\"This case is the 1's column:\")\n","    print(\"Average after the normalization:            1\")\n","    print(\"Standard-deviation after the normalization: 0\")\n","    print()\n","    return v\n","    \n","  for num in range(len(v)):  # Normalize each cell in the vector  \n","    v[num] = (v[num] - ave) / std\n","\n","\n","  # Check if failed\n","  ave_check = np.average(v)\n","  std_check = np.std(v)\n","  assert (0 - epsilon < ave_check < 0 + epsilon) and (1 - epsilon < std_check < 1 + epsilon), \"Error: Normalizing the vector has been failed\"\n","  \n","  print(\"Average after the normalization:            \", int(float('{0:.1f}'.format(ave_check))))\n","  print(\"Standard-deviation after the normalization: \", int(float(std_check)))\n","  print()\n","  \n","  return v\n","  \n","\n","# Show that the ave is 0 and the std is 1 \n","print(\"Show that the average equals 0 and the standard-deviation equals 1 for each column of the data:\\n\")\n","\n","for col in x:\n","  col = normalize_col(col)\n","  \n","\n","y = normalize_col(y)\n","print(\"Average of y after the normalization:            \" , int(float('{0:.1f}'.format(np.average(y)))))\n","print(\"Standard-deviation of y after the normalization: \" , int(float(np.nanstd(y))))\n","print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ywWYa9LKX4e8"},"source":["# **Section B:** Calculate h_theta(x) in linear-regression case"]},{"cell_type":"code","metadata":{"id":"yNO-Ps_0YvZU","executionInfo":{"status":"aborted","timestamp":1632670174039,"user_tz":-180,"elapsed":306,"user":{"displayName":"Elad David","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAAlScA5LNx4a_8K3ad_Q_ugHl3nMQR0bCFHq7=s64","userId":"14654848345289467053"}}},"source":["x = x.T # Transpose for the multiplication\n","def h_theta(theta_v, x):\n","  return x.dot(theta_v) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x3l8-0hwbOY_"},"source":["# **Section C:** Calculate J(theta), aka price function"]},{"cell_type":"code","metadata":{"id":"QfWGefv7bmOh","executionInfo":{"status":"aborted","timestamp":1632670174041,"user_tz":-180,"elapsed":307,"user":{"displayName":"Elad David","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAAlScA5LNx4a_8K3ad_Q_ugHl3nMQR0bCFHq7=s64","userId":"14654848345289467053"}}},"source":["def calc_j(theta_v, x, y):\n","  sum = 0\n","  for i in range(m): # Sum for all rows/ samples\n","    sum += pow(h_theta(theta_v, x[i]) - y[i], 2) # (h(x[i]) - y[i])^2\n","  return (1/(2*m)) * sum"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YkE__nCjgy1v"},"source":["# **Section D:** Calculate grad_J(theta)"]},{"cell_type":"code","metadata":{"id":"glTqdCkZg9Gm","executionInfo":{"status":"aborted","timestamp":1632670174043,"user_tz":-180,"elapsed":309,"user":{"displayName":"Elad David","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAAlScA5LNx4a_8K3ad_Q_ugHl3nMQR0bCFHq7=s64","userId":"14654848345289467053"}}},"source":["def calc_grad_j(theta_v, x, y):\n","  grad_j = h_theta(theta_v, x) \n","  # Calculate grad j - y\n","  for i in range(len(y)):\n","    grad_j[i] -= y[i] # h(x) - y\n","  grad_j = np.dot(x.T, grad_j) / m # x.T * (h(x) - y) / m\n","  \n","  return grad_j "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T4vWgc2H2pAD"},"source":["# **Section E:** Gradient-Descent"]},{"cell_type":"code","metadata":{"id":"BMdnsOku202G","executionInfo":{"status":"aborted","timestamp":1632670174045,"user_tz":-180,"elapsed":311,"user":{"displayName":"Elad David","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAAlScA5LNx4a_8K3ad_Q_ugHl3nMQR0bCFHq7=s64","userId":"14654848345289467053"}}},"source":["def Gradient_Descent(alpha):\n","  theta_v = np.ones((n + 1, 1)) / 3 # Initialize temp theta to (1/3 ... 1/3) (n+1*1)\n","  j_theta = [] # Using for saving all the errors of each iteration (for plotting them later)\n","  j_theta.append(calc_j(theta_v, x, y)) # Calculate j of init theta\n","  iter = 0\n","  \n","  while iter < max_iterations:\n","    # Calculate next theta vector\n","    next_theta = theta_v - (alpha * calc_grad_j(theta_v, x, y))\n","    \n","    # Stop conditions:\n","    if np.linalg.norm(np.subtract(next_theta, theta_v)) < epsilon: # Norm the thetas vector\n","      break\n","    if abs(calc_j(next_theta, x, y) - calc_j(theta_v, x, y)) < delta: # Error (J) delta\n","      break\n","    \n","    theta_v = next_theta\n","    j_theta.append(calc_j(theta_v, x, y)) # Update the error of the calculation\n","    iter += 1\n","  \n","  # Figure:\n","  fig = plt.figure()\n","  plt.plot(j_theta)\n","  plt.suptitle(\"Gradient Descent: J(theta) graph\")\n","  plt.title(\"alpha= \" + str(alpha))\n","  plt.xlabel(\"Iterations\")\n","  plt.ylabel(\"J(theta)\")\n","  plt.show()\n","\n","alpha = 1\n","for i in range(3):\n","  alpha /= 10\n","  Gradient_Descent(alpha)\n","  print()\n","\n","\n","alpha = 0.1 \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ocMmZ_eWlnek"},"source":["**Analysing the results - Optimal alpha:**\n","\n","The plots above have a negative slope, meaning the J(theta) is lower for every iteration added.\n","\n","As you can see, the first graph (when alpha equals 0.1) is convergent to 0 the fastest.\n","\n","On the X-axis the number of iterations is represented, and comparing the number of iterations on each graph, the first graph is still the fastest to be convergented.\n","\n","For conclusion, we should choose the next alpha value (according to the tests above):\n","\n","Alpha = 0.1 מתכנס באופן מהיר ויעיל\n"]},{"cell_type":"markdown","metadata":{"id":"bHKDl1sDXYUW"},"source":["# **Section F:** Stochastic GD algorithm"]},{"cell_type":"code","metadata":{"id":"jxZJwX0EX0ee","executionInfo":{"status":"aborted","timestamp":1632670174046,"user_tz":-180,"elapsed":311,"user":{"displayName":"Elad David","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAAlScA5LNx4a_8K3ad_Q_ugHl3nMQR0bCFHq7=s64","userId":"14654848345289467053"}}},"source":["def stochastic_GD():\n","  alpha = 0.05\n","  theta_v = np.ones((n + 1, 1)) / 3 # Init temp theta to (1/3 ... 1/3) (n+1*1)\n","  j_theta = []\n","  j_theta.append(calc_j(theta_v, x, y)) # Calculate j of init theta\n","  iter = 0\n","  k = 0\n","  i = 1\n","\n","  while iter < max_iterations:\n","    # Calculate new theta considering 1 examle \n","    x_i = x[i : i+1] \n","    y_i = y[i : i+1] \n","    theta_v = theta_v - (alpha * calc_grad_j(theta_v, x_i, y_i))\n","    \n","    j_theta.append(calc_j(theta_v, x, y))\n","    k += 1\n","    i += 1\n","    iter += 1\n","\n","  # Figure:\n","  fig = plt.figure()\n","  plt.plot(j_theta)\n","  plt.suptitle(\"stochastic-GD: J(theta) graph\")\n","  plt.title(\"alpha= \" + str(alpha))\n","  plt.xlabel(\"Iterations\")\n","  plt.ylabel(\"J(theta)\")\n","  plt.show()\n","\n","stochastic_GD()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0LVht2rlX1JL"},"source":["# **Section F:** Mini-Batch algorithm"]},{"cell_type":"code","metadata":{"id":"bDZsLKr8X7qN","executionInfo":{"status":"aborted","timestamp":1632670174047,"user_tz":-180,"elapsed":312,"user":{"displayName":"Elad David","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAAlScA5LNx4a_8K3ad_Q_ugHl3nMQR0bCFHq7=s64","userId":"14654848345289467053"}}},"source":["def mini_batch():\n","  theta_v = np.ones((n + 1, 1)) / 3 # Init temp theta to (1/3 ... 1/3) (n+1*1)\n","  j_theta = []\n","  j_theta.append(calc_j(theta_v, x, y)) # Calculate j of init theta\n","  T = 30 # Num of groups (~100 for each batch; 3047 samples total)\n","  N = int(m/T) # Group size\n","  iter = 0\n","  k = 0\n","\n","  while iter < max_iterations:\n","    i_from = (k * N) % m\n","    i_to = ((k + 1) * N - 1) % m\n","    x_i = x[i_from : i_to] # slice just rows\n","    y_i = y[i_from : i_to]\n","    next_theta = theta_v - (alpha * calc_grad_j(theta_v, x_i, y_i))\n","    \n","    # Stop conditions:\n","    if np.linalg.norm(np.subtract(next_theta, theta_v)) < epsilon: \n","      break\n","    if abs(calc_j(next_theta, x, y) - calc_j(theta_v, x, y)) < delta:\n","      break\n","    \n","    theta_v = next_theta\n","    j_theta.append(calc_j(theta_v, x, y))\n","    k += 1\n","    \n","    \n","    iter += 1\n","\n","  # Figure:\n","  fig = plt.figure()\n","  plt.plot(j_theta)\n","  plt.suptitle(\"Mini-Batch: J(theta) graph\")\n","  plt.title(\"alpha= \" + str(alpha))\n","  plt.xlabel(\"Iterations\")\n","  plt.ylabel(\"J(theta)\")\n","  plt.show()\n","\n","mini_batch()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9bBzk8ZxX88k"},"source":["# **Section F (cont.):** Analyse run-time"]},{"cell_type":"code","metadata":{"id":"po2DS4h_kW2B","executionInfo":{"status":"aborted","timestamp":1632670174051,"user_tz":-180,"elapsed":17,"user":{"displayName":"Elad David","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAAlScA5LNx4a_8K3ad_Q_ugHl3nMQR0bCFHq7=s64","userId":"14654848345289467053"}}},"source":["def time_function(f, *args):\n","  \"\"\"\n","  Call a function f with args and return the time (in seconds) that it took to execute.\n","  \"\"\"\n","  import time\n","  tic = time.time()\n","  f(*args)\n","  toc = time.time()\n","  return toc - tic\n","\n","s_time = time_function(stochastic_GD,)\n","print('Stochastic GD algorithm took %f seconds' % s_time)\n","\n","print()\n","\n","m_b_time = time_function(mini_batch,)\n","print('Mini-Batch algorithm took %f seconds' % m_b_time)\n"],"execution_count":null,"outputs":[]}]}