{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ex3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"owEAqM6W7Cc_"},"source":["# **Ex3 - Finding a subset of extremely important attributes**\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"RlS5LQv17IXT"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","from sklearn.datasets import load_digits\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import plot_confusion_matrix\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2\n","from sklearn import metrics\n","from sklearn.cluster import KMeans\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import scale\n","import collections"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CLzkqHKa79yA"},"source":["Bring the data from sklearn, normalize and split it:"]},{"cell_type":"code","metadata":{"id":"R7Iaa48v7-uH"},"source":["X_digits, y_digits = load_digits(return_X_y=True)\n","\n","n_samples, n_features = X_digits.shape\n","n_digits = len(np.unique(y_digits))\n","\n","#print(\"n_digits: %d, \\t n_samples %d, \\t n_features %d\" %(n_digits, n_samples, n_features))\n","\n","# normalize\n","x_matrix_hat = scale(X_digits)\n","\n","# split data\n","x_train, x_test, y_train, y_test = train_test_split(x_matrix_hat, y_digits, test_size=0.3, random_state=42)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fbdNkJcz-wJr"},"source":["Get Best C (Constant):"]},{"cell_type":"code","metadata":{"id":"ehbsqaFZ-091"},"source":["c = [0.01, 0.1, 1, 10, 100, 1000, 10000]\n","acc = []\n","best_acc = 0\n","best_c = 0\n","i = 0\n","for i in range(len(c)):\n","    object_lr = LogisticRegression(max_iter=1000, C=c[i], penalty='l2').fit(x_train, y_train)\n","    acc.append(object_lr.score(x_test, y_test))\n","    if acc[i] > best_acc:\n","        best_acc = acc[i]\n","        best_c = c[i]\n","c = best_c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rvOb5iZJ9la2"},"source":["Greedy algorithm:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"38eFcBKg9r1d","executionInfo":{"status":"ok","timestamp":1628438561555,"user_tz":-180,"elapsed":18238,"user":{"displayName":"ענבר שמייה","photoUrl":"","userId":"02964396275905919772"}},"outputId":"ba7ae4ad-a151-4605-fbb0-38e03bd6dbb6"},"source":["f = [] # features\n","\n","while len(f) < 5:  # run until we have got 5 features\n","    max_score = 0  # keeps the highest score\n","    feature_max_score = None  # keeps the feature with the highest score\n","    for i in range(n_features):  # for each feature\n","        if i in f:  # if we chose it already to f, skip\n","            continue\n","       \n","        temp = f.copy()\n","        temp.append(i)\n","        object_lr = LogisticRegression(max_iter=1000, C=c, penalty='l2').fit(x_train[:, temp], y_train) # penalty l2 means we use norma 2\n","        y_test = y_test.astype('int')\n","        score = object_lr.score(x_test[:, temp], y_test)  # calculate score (accuracy on the given test)\n","        \n","        if max_score < score:  # if it's higher than the max score update max score\n","            max_score = score\n","            feature_max_score = i\n","    f.append(feature_max_score)  # add the feature the got the highest score\n","\n","# object_lr = LogisticRegression(max_iter=1000, C=c, penalty='l2').fit(x_train[:, f], y_train)\n","# prediction = object_lr.predict(x_test[:, f])  # predict with the features we found\n","\n","#greedy_pred = prediction\n","\n","print(\"Greedy Algorithm - Five most important features: {}\".format(f))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["21\n","42\n","36\n","26\n","61\n","Greedy Algorithm - Five most important features: [21, 42, 36, 26, 61]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iyoC0TTaBHOZ"},"source":["MI algotithm:"]},{"cell_type":"code","metadata":{"id":"lwr3bJZCBkv4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628435602720,"user_tz":-180,"elapsed":827,"user":{"displayName":"ענבר שמייה","photoUrl":"","userId":"02964396275905919772"}},"outputId":"4e87f701-f524-454e-d7ec-9d7555a83b3a"},"source":["f = [] # features\n","eleventosixteen = fivetoten = zerotofour = 0\n","arr_y = [0] * 10  #ten options \n","\n","for i in range(n_features):  # for each feature\n","  if 0 <= i <= 4:\n","    zerotofour += 1\n","  if 5 <= i <= 10:\n","    fivetoten += 1\n","  if 11 <= i <= 16:\n","    eleventosixteen += 1\n","\n","X_new = SelectKBest(chi2, k=5).fit_transform(X_digits, y_digits) # using probability and MI alg\n"," \n","# checking which colunms did we get in x_new\n","X_digits_T = X_digits.T\n","X_new_T = X_new.T\n","compare = lambda x, y: collections.Counter(x) == collections.Counter(y)\n","\n","def get_col_num(col):\n","  for i in range(len(X_digits_T)):\n","   \n","    if compare(X_digits_T[i], col):\n","      return i\n","\n","for i in range(len(X_new_T)):\n","  f.append(get_col_num(X_new_T[i]))\n","\n","print(\"MI Algorithm - Five most important features: {}\".format(f))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MI Algorithm - Five most important features: [33, 34, 42, 43, 54]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bnm3T8Lkzj5-"},"source":["**Analysing the results:**\n","\n","The greedy algorithm takes the most important and meaningful features, so the results are the accurate as possible.\n","The MI (mutual information) algorithm takes the features with the highest importability of y (the results).\n","\n","The results above show that:\n","\n","The greedy algorithm chose the features with the highest corelation  between the input and the output of the test (the first feature that added is number 21), than the algorithm chose the next feature regarding its first choice (what feature resulting the most accurate importability of y, by adding it to the calculation within the chosen feature[s]?). It seems like the algorithm passes on many combinations of features, that way we might miss a better combination of features.\n","\n","The MI algorithm chose the features with the highest corelation between the input and the output (of the test), means these features have higher probabilities than the other that the algorithm didn't choose eventually.\n","\n","\n","By testing the results, we can learn that the MI algorithm is more accurate, because the greedy algorithm passes on many features' combinations that might be better, while probabilities are the same (doesn't matter what the algorithm chose before)."]}]}